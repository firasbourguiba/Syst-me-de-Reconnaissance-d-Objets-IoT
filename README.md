# Syst-me-de-Reconnaissance-d-Objets-IoT
SystÃ¨me de Commande Gestuelle Vocale PersonnalisÃ©e
markdown
Copier
Modifier
# ğŸ¤– SystÃ¨me de Commande Gestuelle Vocale PersonnalisÃ©e

## ğŸ¯ Objectif du projet

Ce projet permet Ã  un utilisateur de contrÃ´ler des messages vocaux Ã  lâ€™aide de gestes personnalisÃ©s capturÃ©s via une webcam. Il est particuliÃ¨rement destinÃ© Ã  des personnes en situation de handicap vocal pour sâ€™exprimer simplement.

## ğŸ§  Fonctionnement

Le systÃ¨me utilise :
- ğŸ–ï¸ MediaPipe pour dÃ©tecter les positions des mains
- ğŸ¤– Un modÃ¨le IA entraÃ®nÃ© avec `scikit-learn`
- ğŸ”Š SynthÃ¨se vocale (text-to-speech) avec `pyttsx3`
- ğŸŒ Une interface Flask pour interagir facilement

---

## ğŸ§± Architecture des fichiers

projet_reconnaissance_objets/
â”œâ”€â”€ data/ # DonnÃ©es capturÃ©es
â”‚ â”œâ”€â”€ gestures.csv # DonnÃ©es d'entraÃ®nement
â”‚ â”œâ”€â”€ gestures_to_phrases.csv # Mappage gestes â†’ messages vocaux
â”‚ â””â”€â”€ history.txt # Historique des dÃ©tections
â”œâ”€â”€ models/
â”‚ â””â”€â”€ sign_model.pkl # ModÃ¨le IA entraÃ®nÃ©
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ collect_gestures.py # Script de collecte de gestes
â”‚ â”œâ”€â”€ train_model.py # EntraÃ®nement du modÃ¨le
â”‚ â”œâ”€â”€ predict_translator.py # Traduction geste â†’ vocal
â”‚ â””â”€â”€ web/ # Interface Flask
â”‚ â”œâ”€â”€ app.py # Routes Flask
â”‚ â””â”€â”€ templates/
â”‚ â””â”€â”€ index.html # Interface utilisateur
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

yaml
Copier
Modifier

---

## âš™ï¸ Installation

1. **Cloner le projet**
```bash
git clone <URL_DU_REPO>
cd projet_reconnaissance_objets
CrÃ©er un environnement virtuel

bash
Copier
Modifier
python -m venv .venv
source .venv/Scripts/activate  # (Windows)
Installer les dÃ©pendances

bash
Copier
Modifier
pip install -r requirements.txt
Si requirements.txt nâ€™existe pas, installe manuellement :

bash
Copier
Modifier
pip install opencv-python mediapipe scikit-learn pyttsx3 pandas flask
ğŸš€ DÃ©marrage rapide
1. Collecter tes propres gestes
bash
Copier
Modifier
python src/collect_gestures.py
â¡ï¸ Enregistre un geste, puis donne-lui un nom (ex: S1, STOP, HELP)

2. DÃ©finir les messages associÃ©s
ğŸ“„ Modifier le fichier :

csv
Copier
Modifier
# data/gestures_to_phrases.csv
label,message
S1,Je suis en danger
S2,Appelez Ã  lâ€™aide sâ€™il vous plaÃ®t
3. EntraÃ®ner le modÃ¨le IA
bash
Copier
Modifier
python src/train_model.py
4. Lancer la prÃ©diction avec vocalisation
bash
Copier
Modifier
python src/predict_translator.py
ğŸ§  Lâ€™IA reconnaÃ®t ton geste â†’ ğŸ”Š lit le message correspondant

ğŸŒ Optionnel : interface web (dashboard)
Lancer :

bash
Copier
Modifier
python src/web/app.py
Ouvrir dans le navigateur :
http://localhost:5000

âœ… FonctionnalitÃ©s terminÃ©es
âœ… DÃ©tection de gestes personnalisÃ©s

âœ… SystÃ¨me de synthÃ¨se vocale

âœ… EntraÃ®nement dynamique du modÃ¨le

âœ… Interface Web (Dashboard + Statistiques + Export CSV)

âœ… RÃ©initialisation et collecte intuitive

ğŸ“Š Technologies utilisÃ©es
Python 3.10+

OpenCV / MediaPipe

Scikit-learn

pyttsx3 (offline voice)

Flask

Bootstrap + Chart.js

ğŸ’¡ AmÃ©liorations futures
ğŸ” Reconnaissance de gestes dynamiques (par mouvement)

ğŸ—‚ï¸ Interface web pour ajouter/Ã©diter les messages

ğŸ“± DÃ©ploiement Raspberry Pi / Interface mobile

ğŸ‘¨â€ğŸ’» Auteur
Nom : Firas
Ã‰cole : Ynov - B2 Informatique
Projet : SystÃ¨me de reconnaissance gestuelle et vocale











 DÃ©tails des bibliothÃ¨ques et pourquoi elles sont nÃ©cessaires :

BibliothÃ¨que	UtilitÃ©
opencv-python	Capture et traitement dâ€™images
numpy	Calculs numÃ©riques (utilisÃ© par OpenCV & AI)
tensorflow	ModÃ¨le dâ€™IA pour la reconnaissance dâ€™objets
torch & torchvision	PyTorch (alternative Ã  TensorFlow pour lâ€™IA)
flask & flask-cors	Serveur API pour communiquer avec Raspberry Pi
sqlite3	Base de donnÃ©es locale
pandas	Manipulation des donnÃ©es
matplotlib	Visualisation des donnÃ©es et rÃ©sultats
requests	Communication avec API distante
pyyaml	Gestion des fichiers de configuration
